{"paragraphs":[{"text":"%pyspark\nfrom pyspark.sql.types import *\n#from pyspark.sql.types.DataType import *\nfrom pyspark.sql.functions import split, regexp_extract, lit, concat, when, col, to_date, date_format\n\n","user":"hduser","dateUpdated":"2019-02-04T03:01:15-0600","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1548879326609_-1945750354","id":"20190130-093755_1666566592","dateCreated":"2019-01-30T14:15:26-0600","dateStarted":"2019-02-04T03:01:15-0600","dateFinished":"2019-02-04T03:01:16-0600","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:107"},{"text":"%pyspark\n\ndf=hour_13\n\nweather_h=weather_hourly_darksky.withColumn(\"DATETIME\", weather_hourly_darksky.DATETIME.cast(StringType()))\ndf=df.drop(df.ID)                        \ndf=df.join(information_households.select([\"METER_ID\",\"CAT\"]), df.METER_ID==information_households.METER_ID).drop(information_households.METER_ID)\nhour_13.unpersist()\n","user":"hduser","dateUpdated":"2019-02-04T03:01:16-0600","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"DataFrame[ID: int, METER_ID: string, TIMESTAMP: timestamp, DATE: date, HD: boolean, ENERGY: double]\n"}]},"apps":[],"jobName":"paragraph_1548879326610_1640629044","id":"20190130-093929_1129075817","dateCreated":"2019-01-30T14:15:26-0600","dateStarted":"2019-02-04T03:01:16-0600","dateFinished":"2019-02-04T03:01:16-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"text":"%pyspark\nmeter_1h=df\nmeter_1h.cache()\n#to filter by month and user\n#month=11\n#user='MAC000002'\n#meter_1h=df.filter(col(\"METER_ID\")==user)\n#meter_1h=meter_1h.filter((date_format(to_date(col('Date'), \"yyyy-MM-dd\"),'MM')==month))\n\n#meter_1h.show(15)\n#print(meter_1h.count())","user":"hduser","dateUpdated":"2019-02-04T03:01:16-0600","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"DataFrame[METER_ID: string, TIMESTAMP: timestamp, DATE: date, HD: boolean, ENERGY: double, CAT: int]\n"}]},"apps":[],"jobName":"paragraph_1548879326612_1565205802","id":"20190130-094300_2046229556","dateCreated":"2019-01-30T14:15:26-0600","dateStarted":"2019-02-04T03:01:16-0600","dateFinished":"2019-02-04T03:02:24-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"text":"%pyspark\n\n#TIMELINE BY USER WITH WEATHER DATA, GRANULARITY 1h\n\nmeter_weather_1h=meter_1h.join(weather_h.select([\"DATETIME\",\"TEMPERATURE\",\"PRESSURE\",\"HUMIDITY\"]), meter_1h.TIMESTAMP==weather_h.DATETIME).drop(weather_h.DATETIME).drop(weather_h.DATE)\n\n#meter_weather_1h.show(10)\n#print(meter_weather_1h.count())\n#meter_1h.unpersist()\n#meter_weather_1h.cache()\n#use next to export dataframe to csv\n#meter_weather_1h_tx = meter_weather_1h.select(row_number().over(Window().orderBy(\"METER_ID\",\"TIMESTAMP\")).alias(\"ID\"), col(\"*\")) \n#meter_weather_1h_tx.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs://big0.iie.org.mx:9000/JSGP/datasets/london/analytics/meter_weather_1h_tx.csv\")","user":"hduser","dateUpdated":"2019-02-04T03:50:49-0600","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1548879326613_-1372903227","id":"20190130-094328_1484881370","dateCreated":"2019-01-30T14:15:26-0600","dateStarted":"2019-02-04T03:50:49-0600","dateFinished":"2019-02-04T03:50:49-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"text":"%pyspark\nmeter_weather_1h.printSchema()","user":"hduser","dateUpdated":"2019-02-04T03:50:52-0600","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- METER_ID: string (nullable = true)\n |-- TIMESTAMP: timestamp (nullable = true)\n |-- DATE: date (nullable = true)\n |-- HD: boolean (nullable = false)\n |-- ENERGY: double (nullable = true)\n |-- CAT: integer (nullable = false)\n |-- TEMPERATURE: double (nullable = true)\n |-- PRESSURE: double (nullable = false)\n |-- HUMIDITY: double (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1549273822096_1898695287","id":"20190204-035022_1722962594","dateCreated":"2019-02-04T03:50:22-0600","dateStarted":"2019-02-04T03:50:52-0600","dateFinished":"2019-02-04T03:50:52-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"text":"%pyspark\n#TIMELINE BY CATEGORY (SUM AND AVG), GRANULARITY 1h\n\n#cat_1h=df\n\n#to filter by month\n#month=11\n#cat_1h=cat_1h.filter((date_format(to_date(col('Date'), \"yyyy-MM-dd\"),'MM')==month)) \n\ncat_1h=(df.groupBy('CAT','TIMESTAMP','HD')\n                .agg(sqlFunctions.sum(\"ENERGY\").alias(\"e_sum\"),sqlFunctions.avg(\"ENERGY\").alias(\"e_avg\"))\n                .orderBy(\"CAT\",\"TIMESTAMP\"))\n#cat_1h.show(10)\n#print(cat_1h.count())\n\n#use next to export dataframe to csv\n#cat_1h = cat_1h.select(row_number().over(Window().orderBy(\"CAT\",\"Timestamp\")).alias(\"ID\"), col(\"*\")) \n#split_df.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs://big0.iie.org.mx:9000/JSGP/datasets/london/analytics/ts_cat_hour_2013.csv\")","user":"hduser","dateUpdated":"2019-02-04T03:22:31-0600","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1548879326615_-1272576801","id":"20190130-094340_389632674","dateCreated":"2019-01-30T14:15:26-0600","dateStarted":"2019-02-04T03:22:31-0600","dateFinished":"2019-02-04T03:22:31-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"text":"%pyspark\n#TIMELINE BY CATEGORY (SUM AND AVG) WITH WEATHER DATA, GRANULARITY 1h\n\ncat_weather_1h=cat_1h.join(weather_h, cat_1h.TIMESTAMP==weather_h.DATETIME)\ncat_weather_1h=cat_weather_1h.drop(weather_h.DATETIME).drop(weather_h.ID).drop(weather_h.DATE)\ncat_weather_1h.cache()\ncat_weather_1h_tx = cat_weather_1h.select(row_number().over(Window().orderBy(\"CAT\",\"TIMESTAMP\")).alias(\"ID\"), col(\"*\")) \ncat_weather_1h_tx.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs://big0.iie.org.mx:9000/JSGP/datasets/london/analytics/cat_weather_1h_tx.csv\")\n#cat_weather_1h.show(10)\n#print(cat_weather_1h.count())\n","user":"hduser","dateUpdated":"2019-02-04T03:31:42-0600","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1548879326616_-1220015579","id":"20190130-094428_1647892639","dateCreated":"2019-01-30T14:15:26-0600","dateStarted":"2019-02-04T03:31:42-0600","dateFinished":"2019-02-04T03:31:48-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"text":"%pyspark\ncat_weather_1h.printSchema()","user":"hduser","dateUpdated":"2019-02-04T03:29:00-0600","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- CAT: integer (nullable = false)\n |-- TIMESTAMP: timestamp (nullable = true)\n |-- HD: boolean (nullable = false)\n |-- e_sum: double (nullable = true)\n |-- e_avg: double (nullable = true)\n |-- TEMPERATURE: double (nullable = true)\n |-- PRESSURE: double (nullable = false)\n |-- HUMIDITY: double (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1549272529080_-1516808424","id":"20190204-032849_364589184","dateCreated":"2019-02-04T03:28:49-0600","dateStarted":"2019-02-04T03:29:00-0600","dateFinished":"2019-02-04T03:29:00-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"text":"%pyspark\n#Matrix by meter by date, granularity by hour, JUST TUESDAYS AND WEDNESDAYS\n\nhour_mat=hour_df.drop(hour_df.ID).drop(hour_df.MAX_E)\nhour_mat=hour_mat.join(information_households.select([\"METER_ID\",\"CAT\"]), hour_mat.METER_ID==information_households.METER_ID).drop(information_households.METER_ID)\nhour_mat=hour_mat.filter(((date_format(hour_mat['DATE'], 'u')==2 )|(date_format(hour_mat['DATE'], 'u')==3)) & (hour_mat.HD==False))\nhour_mat=hour_mat.drop(hour_df.HD)\nhour_mat=hour_mat.orderBy(\"METER_ID\",\"DATE\")\nhour_mat.cache()\n#use next to export dataframe to csv\nhour_mat_tx = hour_mat.select(row_number().over(Window().orderBy(\"METER_ID\")).alias(\"ID\"), col(\"*\"))\n#hour_mat_tx.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs://big0.iie.org.mx:9000/JSGP/datasets/london/analytics/hour_mat_tx.csv\")\n#hour_mat.show(10)\n#print(hour_mat.count())","user":"hduser","dateUpdated":"2019-02-04T01:46:30-0600","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1548879326617_1556152718","id":"20190130-094442_719564161","dateCreated":"2019-01-30T14:15:26-0600","dateStarted":"2019-02-04T01:46:30-0600","dateFinished":"2019-02-04T01:46:51-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"text":"%pyspark\n#Matrix by meter, granularity by hour, JUST TUESDAYS AND WEDNESDAYS \n\nhour_mat_agg=(hour_mat.groupBy('METER_ID')\n                .agg(round(sqlFunctions.avg(\"H0\"),2).alias(\"H0\"),\n                    round(sqlFunctions.avg(\"H1\"),2).alias(\"H1\"),\n                    round(sqlFunctions.avg(\"H2\"),2).alias(\"H2\"),\n                    round(sqlFunctions.avg(\"H3\"),2).alias(\"H3\"),\n                    round(sqlFunctions.avg(\"H4\"),2).alias(\"H4\"),\n                    round(sqlFunctions.avg(\"H5\"),2).alias(\"H5\"),\n                    round(sqlFunctions.avg(\"H6\"),2).alias(\"H6\"),\n                    round(sqlFunctions.avg(\"H7\"),2).alias(\"H7\"),\n                    round(sqlFunctions.avg(\"H8\"),2).alias(\"H8\"),\n                    round(sqlFunctions.avg(\"H9\"),2).alias(\"H9\"),\n                    round(sqlFunctions.avg(\"H10\"),2).alias(\"H10\"),\n                    round(sqlFunctions.avg(\"H11\"),2).alias(\"H11\"),\n                    round(sqlFunctions.avg(\"H12\"),2).alias(\"H12\"),\n                    round(sqlFunctions.avg(\"H13\"),2).alias(\"H13\"),\n                    round(sqlFunctions.avg(\"H14\"),2).alias(\"H14\"),\n                    round(sqlFunctions.avg(\"H15\"),2).alias(\"H15\"),\n                    round(sqlFunctions.avg(\"H16\"),2).alias(\"H16\"),\n                    round(sqlFunctions.avg(\"H17\"),2).alias(\"H17\"),\n                    round(sqlFunctions.avg(\"H18\"),2).alias(\"H18\"),\n                    round(sqlFunctions.avg(\"H19\"),2).alias(\"H19\"),\n                    round(sqlFunctions.avg(\"H20\"),2).alias(\"H20\"),\n                    round(sqlFunctions.avg(\"H21\"),2).alias(\"H21\"),\n                    round(sqlFunctions.avg(\"H22\"),2).alias(\"H22\"),\n                    round(sqlFunctions.avg(\"H23\"),2).alias(\"H23\"),\n                    sqlFunctions.avg(\"CAT\").cast(IntegerType()).alias(\"CAT\"))\n                .orderBy(\"METER_ID\"))\n                \nhour_mat_agg.cache()                \n#use next to export dataframe to csv\nhour_mat_agg_tx = hour_mat_agg.select(row_number().over(Window().orderBy(\"METER_ID\")).alias(\"ID\"), col(\"*\")) \n#hour_mat_agg_tx.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs://big0.iie.org.mx:9000/JSGP/datasets/london/analytics/hour_mat_agg_tx.csv\")\n#hour_mat_agg.show(10)\n#print(hour_mat_agg.count())","user":"hduser","dateUpdated":"2019-02-04T01:48:31-0600","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Fail to execute line 34: hour_mat_agg_tx.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs://big0.iie.org.mx:9000/JSGP/datasets/london/analytics/hour_mat_agg_tx.csv\")\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2052256690995687267.py\", line 380, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 34, in <module>\n  File \"/usr/local/bigdata/spark-2.3.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 705, in save\n    self._jwrite.save(path)\n  File \"/usr/local/bigdata/spark-2.3.2-bin-hadoop2.6/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/local/bigdata/spark-2.3.2-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\nAnalysisException: u'path hdfs://big0.iie.org.mx:9000/JSGP/datasets/london/analytics/hour_mat_agg_tx.csv already exists.;'\n"}]},"apps":[],"jobName":"paragraph_1548879326618_-2113227885","id":"20190130-094612_1299895641","dateCreated":"2019-01-30T14:15:26-0600","dateStarted":"2019-02-04T01:46:51-0600","dateFinished":"2019-02-04T01:47:05-0600","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:116"},{"text":"%pyspark\nhouse=spark.read.csv(\"hdfs://big0.iie.org.mx:9000/JSGP/datasets/london/analytics/households_clusters.csv\", header=True, schema = StructType([\n                        StructField('ID', IntegerType(), True),\n                        StructField('METER_ID', StringType(), True),\n                        StructField('STD_TOU', StringType(), True),\n                        StructField('ACORN_CAT', IntegerType(), True),\n                        StructField('ACORN_CAT_NAME', StringType(), True),\n                        StructField('ACORN_GROUP', StringType(), True),\n                        StructField('CAT', IntegerType(), True),\n                        StructField('CAT_NAME', StringType(), True),\n                        StructField('CATC', IntegerType(), True),\n                        StructField('CATC_NAME', StringType(), True)]))\n                        \ndf=hour_ts.join(house.drop(\"ID\"),hour_ts.METER_ID==house.METER_ID).drop(hour_ts.METER_ID)\n\ncat_1h= df.groupBy(\"CAT\",\"CAT_NAME\",\"TIMESTAMP\").agg(sqlFunctions.sum(\"ENERGY\").alias(\"e_sum\"),sqlFunctions.avg(\"ENERGY\").alias(\"e_avg\")).orderBy(\"CAT\",\"TIMESTAMP\")\ncat_1h_tx = cat_1h.select(row_number().over(Window().orderBy(\"CAT\",\"TIMESTAMP\")).alias(\"ID\"), col(\"*\")) \n#cat_1h_tx.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs://big0.iie.org.mx:9000/JSGP/datasets/london/analytics/cat_1h_tx.csv\")\n\n\n\n","user":"hduser","dateUpdated":"2019-02-04T03:11:57-0600","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1549136894394_-1584596451","id":"20190202-134814_1973066176","dateCreated":"2019-02-02T13:48:14-0600","dateStarted":"2019-02-03T12:50:38-0600","dateFinished":"2019-02-03T12:50:39-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:117"},{"text":"%pyspark\ncatc_1h= df.groupBy(\"CATC\",\"CATC_NAME\",\"TIMESTAMP\").agg(sqlFunctions.sum(\"ENERGY\").alias(\"e_sum\"),sqlFunctions.avg(\"ENERGY\").alias(\"e_avg\")).orderBy(\"CATC\",\"TIMESTAMP\")\ncatc_1h_tx = catc_1h.select(row_number().over(Window().orderBy(\"CATC\",\"TIMESTAMP\")).alias(\"ID\"), col(\"*\")) \n#catc_1h_tx.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs://big0.iie.org.mx:9000/JSGP/datasets/london/analytics/catc_1h_tx.csv\")","user":"hduser","dateUpdated":"2019-02-04T03:12:08-0600","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1549148453537_727795764","id":"20190202-170053_175802440","dateCreated":"2019-02-02T17:00:53-0600","dateStarted":"2019-02-03T12:50:39-0600","dateFinished":"2019-02-03T12:50:39-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:118"},{"text":"%pyspark\n","user":"hduser","dateUpdated":"2019-02-03T12:50:39-0600","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549219839014_-1585500841","id":"20190203-125039_328543738","dateCreated":"2019-02-03T12:50:39-0600","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:119"}],"name":"JSGP/london/analytics/data","id":"2E3DRET59","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}